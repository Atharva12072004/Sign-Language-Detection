# Sign-Language-Detection
Sign Language Detection: This project utilizes machine learning and computer vision techniques to recognize and translate sign language gestures into text and speech. By providing real-time gesture recognition through a user-friendly interface, it aims to enhance communication accessibility for the deaf and hard-of-hearing community. 

Created By Atharva Kailas Harane

# OVERVIEW OF PROJECT
This project consists of three buttons first is "capture & train gesture" which helps to capture the real-time ISL action by accessing the user's camera, then it asks for the name of that specific gesture and saves that file as a .npy file extension. This file will stored in the gesture data folder in this project which is made for storing the dataset. The second button "train model" is used for training the program after training is complete it shows a pop-up message as the gesture is trained successfully. The third button is "Recognise Gesture" which accesses the user's camera to recognize the gestures and it converts those gestures into text and text is shown at the top of the camera input feed as shown in the output section. Note That if data is not available in the gesture data file program doesn't recognize anything!!!!
 

# OUTPUT
![image](https://github.com/user-attachments/assets/24f73088-e690-4ede-aa65-d7147f4f387f)

